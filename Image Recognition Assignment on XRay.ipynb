{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"last_till_now (1).ipynb","provenance":[],"collapsed_sections":["dymeudA4JQ9P"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"eIZAgjuYPD9U","colab":{}},"source":["!pip install PyDrive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N7FoGCoiM7Wj","colab_type":"text"},"source":["**Imports**"]},{"cell_type":"code","metadata":{"id":"CrgKWWVhVx2d","colab_type":"code","colab":{}},"source":["import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQopQjjk0H-T","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","import matplotlib.image as mpimg\n","import pandas as pd\n","import io\n","import numpy as np\n","import regex as re\n","from sklearn.preprocessing import MultiLabelBinarizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jPKjB6jf_57","colab_type":"code","outputId":"32e0c88a-c8d7-4866-d162-469f27d51ac6","executionInfo":{"status":"ok","timestamp":1579849791486,"user_tz":-120,"elapsed":27754,"user":{"displayName":"Giannaras Tyxaioc","photoUrl":"","userId":"14546039083136991725"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iscI_jwYWRKD","colab_type":"code","colab":{}},"source":["!unzip '/content/drive/My Drive/validation-set.zip'\n","!unzip '/content/drive/My Drive/training-set.zip'\n","#!unzip '/content/drive/My Drive/ImageCLEF_test-set.zip'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEZhqI0TEPSJ","colab_type":"code","colab":{}},"source":["mydf = pd.read_csv('train_concepts.csv')\n","valdf = pd.read_csv('val_concepts.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKzyEbfnY_FV","colab_type":"code","outputId":"7b70f107-75c5-440a-a4d7-24fa9902ca5c","executionInfo":{"status":"ok","timestamp":1579856224724,"user_tz":-120,"elapsed":562,"user":{"displayName":"Giannaras Tyxaioc","photoUrl":"","userId":"14546039083136991725"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["#Let's print the four first rows of the dataframe, to get an idea of how the df looks like\n","mydf.head(4)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ROCO_CLEF_07350</td>\n","      <td>C0203126;C0203051</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ROCO_CLEF_19073</td>\n","      <td>C0772294;C0023884;C0221198;C0412555;C0041618</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ROCO_CLEF_60501</td>\n","      <td>C0233492;C2985494;C0262950;C1306232</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ROCO_CLEF_05564</td>\n","      <td>C0521530;C0817096</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             image                                          tags\n","0  ROCO_CLEF_07350                             C0203126;C0203051\n","1  ROCO_CLEF_19073  C0772294;C0023884;C0221198;C0412555;C0041618\n","2  ROCO_CLEF_60501           C0233492;C2985494;C0262950;C1306232\n","3  ROCO_CLEF_05564                             C0521530;C0817096"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"9dzy-CYsEHEX","colab_type":"code","colab":{}},"source":["def listifier(mydf):\n","#Let's turn the tag values from string with internal ';' seperation, to actual lists, and set the image name to index\n","  for i in range(len(mydf['tags'])):\n","    mydf['tags'][i] = mydf['tags'][i].split(';')\n","  return mydf\n","list_mydf = listifier(mydf)\n","list_valdf = listifier(valdf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p7lQPFnWRMEe","colab_type":"code","colab":{}},"source":["#We create a list containing all the training imagefiles\n","train_imagelist = os.listdir('./training-set')\n","train_imagelist = [i for i in train_imagelist if i.endswith('.jpg')]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cVMusqRmGSQ","colab_type":"code","colab":{}},"source":["img = []\n","for i in range(10):\n","    train_list = [train_imagelist[i], mpimg.imread(\"./training-set/\"+train_imagelist[i])]\n","    img.append(train_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ac_fanq2nseW","colab_type":"code","colab":{}},"source":["newfig = plt.figure(figsize=(20,40))\n","for pic in range(10):\n","    plt.subplot(10, 2, pic+1)\n","    plt.imshow(img[pic][1])\n","    plt.title(img[pic][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AoQuiRq-I_q","colab_type":"code","colab":{}},"source":["#Let's import the tag id-name mapper\n","tagdf = pd.read_csv('string_concepts.csv', header=None)\n","#Now let's seperate the tag id and the tag name, name the columns accordingly,set the id as index and prin a few rows.\n","tagdf = tagdf[tagdf.columns[0]].str.split(pat = '\\t',n=0,  expand=True)\n","tagdf.columns = ['tagid', 'tagname']\n","tagdf = tagdf.set_index('tagid')\n","tagdf.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNFYBoL5gsGH","colab_type":"code","colab":{}},"source":["#Now let's print the tags ids and names of the images we plotted\n","for i in range(10):\n","  print(mydf['image'][i])\n","  for j in mydf['tags'][i]:\n","    print('\\t' + j + ' = ' + tagdf['tagname'][j])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_k8s1Wzpfj3","colab_type":"code","colab":{}},"source":["#Let's get the total count of tags and the count of unique tags and the 10 most frequent terms.\n","#We also prin the avg number of tags per image\n","tagunique = []\n","taglist = []\n","for i in range(len(mydf['image'])):\n","  for j in mydf['tags'][i]:\n","    if j not in tagunique:\n","      tagunique.append(j)\n","    taglist.append(j)\n","print(\"There is a total of \" + str(len(tagunique)) + \" unique values, assigned in total \" + str(len(taglist)) + \" times \\n\" )\n","\n","from collections import Counter\n","print(\"The 6 most common tags, along with their number of occurencies are: \\n\" + str(Counter(taglist).most_common(6)) +'\\n')\n","print(\"The average number of tags per image is \" + str(len(taglist)/len(mydf['image'])))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxMWrFUubDLD","colab_type":"text"},"source":["##One-hot"]},{"cell_type":"markdown","metadata":{"id":"dymeudA4JQ9P","colab_type":"text"},"source":["###One-hot Encoder. Failed attempt\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ynlY2OEyLKGD","colab_type":"text"},"source":["The following attempt would cause an issue of assigning the values to the followin column. This was inherented by the transformation of the numpy array following the encoding, to df. Possible cause of the issue is the reservation of the 0 position by tensorflow. Attempts to reassign, caused shape mismatches among the structures used, or wrong results for the first/last attribute columns."]},{"cell_type":"code","metadata":{"id":"xMwxp5kdLGJF","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","token_index = []\n","for i in mydf['tags']:\n","  for j in i:\n","    if j not in token_index:\n","      token_index.append(j)\n","newar = mydf['tags'].to_numpy()\n","tokenizer = Tokenizer(num_words=(len(token_index)))\n","tokenizer.fit_on_texts(newar)\n","one_hot_results = tokenizer.texts_to_matrix(newar, mode='binary')\n","\n","#tokenizer alphabetically orders the column names, but keeps the initial row order it was given\n","#So, we can simply concat the numpy array outputed with the dataframe we created, including the name and the attributes list of the images\n","newdf = pd.DataFrame(data = one_hot_results, columns = list(tokenizer.word_index.keys()))\n","result1 = pd.concat([mydf, newdf], axis=1, sort=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2g6k7T1bNITl","colab_type":"code","colab":{}},"source":["#Check for assignment validity. Expected 1s\n","for i in range(1,3):\n","  for j in result1['tags'][i]:\n","    j = j.lower()\n","    #print(result[j][i])\n","    print(\"Score for \" + str(result1['tags'][i]) + \" in tag \" + j + ' is ' + str(result1[j][i]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QyLfeWDlOE8h","colab_type":"text"},"source":["###One-hot Encoder. Accepted alternative"]},{"cell_type":"code","metadata":{"id":"8xej2gGTPcbq","colab_type":"code","outputId":"d6062a0b-9685-4204-ceaf-01fa4a571509","executionInfo":{"status":"ok","timestamp":1579856637464,"user_tz":-120,"elapsed":16758,"user":{"displayName":"Giannaras Tyxaioc","photoUrl":"","userId":"14546039083136991725"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["def onehotenc(mydf):\n","  mlb = MultiLabelBinarizer()\n","  multibinarray = mlb.fit_transform(mydf['tags'])\n","  mydf1 = pd.DataFrame(data = multibinarray, columns = mlb.classes_)\n","  result = pd.concat([mydf, mydf1], axis=1, sort=False)\n","  #Let's try a few examples to make sure our values are correctly assigned after the conact. We are expecting all 1s\n","  for i in range(1,3):\n","    for j in result['tags'][i]:\n","      print(\"Score for \" + str(result['tags'][i]) + \" in tag \" + j + ' is ' + str(result[j][i]) )\n","  return(result)\n","#Before implementing our one-hot encoding, we need to append the train and validation set\n","#This is in order to acquire all the available labels, as well as having the same number of label columns in both sets.\n","fulldf = mydf.append(valdf, ignore_index=True)\n","full_result = onehotenc(fulldf)\n","#We will now create two dataframes, for training and validating data respectively. We will use the columns from the appended one-hoted df,\n","#and we will assign to each the accroding data. Given the fact that the order of the rows remain unchanged, we will use the length \n","#parameters to split the data to the two new dfs. \n","hot_train = pd.DataFrame(columns=full_result.columns)\n","hot_valid = pd.DataFrame(columns=full_result.columns)\n","hot_train = full_result.head(len(list_mydf))\n","hot_valid = full_result.iloc[-len(list_valdf):]\n","hot_valid = hot_valid.reset_index(drop = True)\n","#train_result = onehotenc(mydf)\n","#valid_result = onehotenc(valdf)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Score for ['C0772294', 'C0023884', 'C0221198', 'C0412555', 'C0041618'] in tag C0772294 is 1\n","Score for ['C0772294', 'C0023884', 'C0221198', 'C0412555', 'C0041618'] in tag C0023884 is 1\n","Score for ['C0772294', 'C0023884', 'C0221198', 'C0412555', 'C0041618'] in tag C0221198 is 1\n","Score for ['C0772294', 'C0023884', 'C0221198', 'C0412555', 'C0041618'] in tag C0412555 is 1\n","Score for ['C0772294', 'C0023884', 'C0221198', 'C0412555', 'C0041618'] in tag C0041618 is 1\n","Score for ['C0233492', 'C2985494', 'C0262950', 'C1306232'] in tag C0233492 is 1\n","Score for ['C0233492', 'C2985494', 'C0262950', 'C1306232'] in tag C2985494 is 1\n","Score for ['C0233492', 'C2985494', 'C0262950', 'C1306232'] in tag C0262950 is 1\n","Score for ['C0233492', 'C2985494', 'C0262950', 'C1306232'] in tag C1306232 is 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i3qSClUgyXsm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":353},"outputId":"cca40f65-843d-4ce3-cc59-c3abd6e9805a","executionInfo":{"status":"ok","timestamp":1579856742832,"user_tz":-120,"elapsed":522,"user":{"displayName":"Giannaras Tyxaioc","photoUrl":"","userId":"14546039083136991725"}}},"source":["hot_valid.head(3)"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>tags</th>\n","      <th>C0000503</th>\n","      <th>C0000723</th>\n","      <th>C0000726</th>\n","      <th>C0000780</th>\n","      <th>C0000782</th>\n","      <th>C0000790</th>\n","      <th>C0000828</th>\n","      <th>C0000925</th>\n","      <th>C0000934</th>\n","      <th>C0000962</th>\n","      <th>C0001074</th>\n","      <th>C0001162</th>\n","      <th>C0001209</th>\n","      <th>C0001416</th>\n","      <th>C0001428</th>\n","      <th>C0001430</th>\n","      <th>C0001511</th>\n","      <th>C0001558</th>\n","      <th>C0001563</th>\n","      <th>C0001575</th>\n","      <th>C0001577</th>\n","      <th>C0001613</th>\n","      <th>C0001629</th>\n","      <th>C0001632</th>\n","      <th>C0001701</th>\n","      <th>C0001721</th>\n","      <th>C0001811</th>\n","      <th>C0002045</th>\n","      <th>C0002191</th>\n","      <th>C0002374</th>\n","      <th>C0002638</th>\n","      <th>C0002688</th>\n","      <th>C0002691</th>\n","      <th>C0002708</th>\n","      <th>C0002766</th>\n","      <th>C0002771</th>\n","      <th>C0002844</th>\n","      <th>C0002873</th>\n","      <th>...</th>\n","      <th>C4283785</th>\n","      <th>C4283791</th>\n","      <th>C4283904</th>\n","      <th>C4284399</th>\n","      <th>C4284930</th>\n","      <th>C4284931</th>\n","      <th>C4285062</th>\n","      <th>C4285610</th>\n","      <th>C4288160</th>\n","      <th>C4288293</th>\n","      <th>C4288599</th>\n","      <th>C4309954</th>\n","      <th>C4315900</th>\n","      <th>C4316796</th>\n","      <th>C4316915</th>\n","      <th>C4317146</th>\n","      <th>C4318483</th>\n","      <th>C4318616</th>\n","      <th>C4318652</th>\n","      <th>C4318935</th>\n","      <th>C4319192</th>\n","      <th>C4319531</th>\n","      <th>C4321236</th>\n","      <th>C4321335</th>\n","      <th>C4321352</th>\n","      <th>C4321395</th>\n","      <th>C4321408</th>\n","      <th>C4321499</th>\n","      <th>C4329495</th>\n","      <th>C4329648</th>\n","      <th>C4330491</th>\n","      <th>C4331246</th>\n","      <th>C4331837</th>\n","      <th>C4476838</th>\n","      <th>C4479170</th>\n","      <th>C4479578</th>\n","      <th>C4505257</th>\n","      <th>C4505346</th>\n","      <th>C4505362</th>\n","      <th>C4505478</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ROCO_CLEF_64017</td>\n","      <td>[C0001074, C3203359, C0041618]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ROCO_CLEF_39073</td>\n","      <td>[C0027530, C1610719, C0065967, C1704258, C0883...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ROCO_CLEF_18028</td>\n","      <td>[C1265876, C1293134, C0029053, C0034579, C0025...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 5530 columns</p>\n","</div>"],"text/plain":["             image  ... C4505478\n","0  ROCO_CLEF_64017  ...        0\n","1  ROCO_CLEF_39073  ...        0\n","2  ROCO_CLEF_18028  ...        0\n","\n","[3 rows x 5530 columns]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"W2ep0Os0X3FY","colab_type":"code","colab":{}},"source":["def jpgfier(plaindf):\n","  #Adds the .jpg suffix, as the flow from dataframe function requires the input filename to include it.\n","  pd.set_option('mode.chained_assignment', None)\n","  for i in range(len(plaindf['image'])):\n","    plaindf['image'][i] = str(plaindf['image'][i]+'.jpg')\n","  return plaindf\n","train_result = full_result.head(len(mydf))\n","train_result = jpgfier(hot_train)\n","valid_result = jpgfier(hot_valid)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2eXN3jtwHUpn","colab_type":"text"},"source":["##Data load\n","Obviously, importing the whole image datasets based on our raw muscle, will not even work in a tpu-accelerated env. We could try manually resizing and batching, but we can simply use the ImageDataGenerator, which will also be our base for data augmentation happening later on.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pYrsQsn--EBQ"},"source":["###Data Preperation. Alternative #1\n","The following alternative does not require the execution of the one-hot encoder implemented eariler. It offers an extra level of security iver the outpouted result, as not only the number of rows, but the number of classes identified is given as info created by the process. "]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"9cf73944-e25d-4657-d36c-b2d7e2d2d1b9","id":"RUM3YkrH-IY3","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","datagen=ImageDataGenerator(rescale=1/255.)\n","\n","trainit=datagen.flow_from_dataframe(dataframe=train_result,directory=\"./training-set/\",x_col=\"image\",\n","                                    y_col=train_result.columns[2:],class_mode=\"other\",target_size=(100,100))\n","\n","validit=datagen.flow_from_dataframe(dataframe=valid_result,directory=\"./validation-set/\",x_col=\"image\",\n","                                    y_col=valid_result.columns[2:],class_mode=\"other\",target_size=(100,100))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 56629 validated image filenames.\n","Found 14157 validated image filenames.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U_oSLZNA9XEW","colab_type":"text"},"source":["###Data Preperation. Alternative #2\n","The following alternative does not require the execution of the one-hot encoder implemented eariler. It offers an extra level of security iver the outpouted result, as not only the number of rows, but the number of classes identified is given as info created by the process. "]},{"cell_type":"code","metadata":{"id":"pryFuvzk-pSa","colab_type":"code","outputId":"943dc0a8-f0a9-41ea-cbdf-a035f68f01dc","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","datagen=ImageDataGenerator(rescale=1/255.)\n","trainit=datagen.flow_from_dataframe( dataframe=train_result, directory=\"./training-set/\",x_col=\"image\", \n","                                    y_col=train_result.columns[1], class_mode=\"categorical\", target_size=(100,100))\n","\n","validit=datagen.flow_from_dataframe( dataframe=valid_result, directory=\"./validation-set/\",x_col=\"image\", \n","                                    y_col=valid_result.columns[1], class_mode=\"categorical\", target_size=(100,100))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 14157 validated image filenames belonging to 3233 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ho8430njDsM2","colab_type":"text"},"source":["##Baseline \n","As our baseline we will leverage the dummygenerator, having as strategy the stratified "]},{"cell_type":"code","metadata":{"id":"oqmV_Y4eCwW5","colab_type":"code","colab":{}},"source":["from sklearn.dummy import DummyClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","trainx = train_result['image']\n","trainy = train_result.iloc[:,2:].to_numpy()\n","validx = valid_result['image']\n","dummy_clf = DummyClassifier(strategy=\"stratified\")\n","validy = valid_result.iloc[:,2:].to_numpy()\n","dummy_clf.fit(trainx,trainy)\n","yres = dummy_clf.predict(validx)\n","f1_score(validy, yres)\n","f1_score(validy, yres, average='micro')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJEvc7NwG50m","colab_type":"text"},"source":["##First neural model. \n","Plain model from scratch"]},{"cell_type":"code","metadata":{"id":"FpWheRiOG5Ai","colab_type":"code","outputId":"7e405a27-2ade-411f-e4e3-94ab8df6e1ea","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1579862378512,"user_tz":-120,"elapsed":904,"user":{"displayName":"Giannaras Tyxaioc","photoUrl":"","userId":"14546039083136991725"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","\n","model = Sequential()\n","model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=(100,100,3)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Conv2D(filters=64, kernel_size=(5, 5), activation=\"relu\"))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(25, activation='sigmoid'))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eYjdPttzp-zr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}